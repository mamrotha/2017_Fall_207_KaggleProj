{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/random-acts-of-pizza\n",
    "\n",
    "**Data fields:**\n",
    "\n",
    "\"giver_username_if_known\": Reddit username of giver if known, i.e. the person satisfying the request (\"N/A\" otherwise).\n",
    "\n",
    "\"number_of_downvotes_of_request_at_retrieval\": Number of downvotes at the time the request was collected.\n",
    "\n",
    "\"number_of_upvotes_of_request_at_retrieval\": Number of upvotes at the time the request was collected.\n",
    "\n",
    "\"post_was_edited\": Boolean indicating whether this post was edited (from Reddit).\n",
    "\n",
    "\"request_id\": Identifier of the post on Reddit, e.g. \"t3_w5491\".\n",
    "\n",
    "\"request_number_of_comments_at_retrieval\": Number of comments for the request at time of retrieval.\n",
    "\n",
    "\"request_text\": Full text of the request.\n",
    "\n",
    "\"request_text_edit_aware\": Edit aware version of \"request_text\". We use a set of rules to strip edited comments indicating the success of the request such as \"EDIT: Thanks /u/foo, the pizza was delicous\".\n",
    "\n",
    "\"request_title\": Title of the request.\n",
    "\n",
    "\"requester_account_age_in_days_at_request\": Account age of requester in days at time of request.\n",
    "\n",
    "\"requester_account_age_in_days_at_retrieval\": Account age of requester in days at time of retrieval.\n",
    "\n",
    "\"requester_days_since_first_post_on_raop_at_request\": Number of days between requesters first post on RAOP and this request (zero if requester has never posted before on RAOP).\n",
    "\n",
    "\"requester_days_since_first_post_on_raop_at_retrieval\": Number of days between requesters first post on RAOP and time of retrieval.\n",
    "\n",
    "\"requester_number_of_comments_at_request\": Total number of comments on Reddit by requester at time of request.\n",
    "\n",
    "\"requester_number_of_comments_at_retrieval\": Total number of comments on Reddit by requester at time of retrieval.\n",
    "\n",
    "\"requester_number_of_comments_in_raop_at_request\": Total number of comments in RAOP by requester at time of request.\n",
    "\n",
    "\"requester_number_of_comments_in_raop_at_retrieval\": Total number of comments in RAOP by requester at time of retrieval.\n",
    "\n",
    "\"requester_number_of_posts_at_request\": Total number of posts on Reddit by requester at time of request.\n",
    "\n",
    "\"requester_number_of_posts_at_retrieval\": Total number of posts on Reddit by requester at time of retrieval.\n",
    "\n",
    "\"requester_number_of_posts_on_raop_at_request\": Total number of posts in RAOP by requester at time of request.\n",
    "\n",
    "\"requester_number_of_posts_on_raop_at_retrieval\": Total number of posts in RAOP by requester at time of retrieval.\n",
    "\n",
    "\"requester_number_of_subreddits_at_request\": The number of subreddits in which the author had already posted in at the time of request.\n",
    "\n",
    "\"requester_received_pizza\": Boolean indicating the success of the request, i.e., whether the requester received pizza.\n",
    "\n",
    "\"requester_subreddits_at_request\": The list of subreddits in which the author had already posted in at the time of request.\n",
    "\n",
    "\"requester_upvotes_minus_downvotes_at_request\": Difference of total upvotes and total downvotes of requester at time of request.\n",
    "\n",
    "\"requester_upvotes_minus_downvotes_at_retrieval\": Difference of total upvotes and total downvotes of requester at time of retrieval.\n",
    "\n",
    "\"requester_upvotes_plus_downvotes_at_request\": Sum of total upvotes and total downvotes of requester at time of request.\n",
    "\n",
    "\"requester_upvotes_plus_downvotes_at_retrieval\": Sum of total upvotes and total downvotes of requester at time of retrieval.\n",
    "\n",
    "\"requester_user_flair\": Users on RAOP receive badges (Reddit calls them flairs) which is a small picture next to their username. In our data set the user flair is either None (neither given nor received pizza, N=4282), \"shroom\" (received pizza, but not given, N=1306), or \"PIF\" (pizza given after having received, N=83).\n",
    "\n",
    "\"requester_username\": Reddit username of requester.\n",
    "\n",
    "\"unix_timestamp_of_request\": Unix timestamp of request (supposedly in timezone of user, but in most cases it is equal to the UTC timestamp -- which is incorrect since most RAOP users are from the USA).\n",
    "\n",
    "\"unix_timestamp_of_request_utc\": Unit timestamp of request in UTC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ADD METRICS\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Julia sample code**\n",
    "\n",
    "using DataFrames\n",
    "using MachineLearning\n",
    "using JSON\n",
    "\n",
    "function read_data(file_name)\n",
    "    f = open(file_name)\n",
    "    json = JSON.parse(readall(f))\n",
    "    close(f)\n",
    "\n",
    "    colnames = keys(json[1])\n",
    "    columns  = Any[[json[i][name] for i=1:length(json)] for name=colnames]\n",
    "    DataFrame(columns, Symbol[name for name=colnames])\n",
    "end\n",
    "\n",
    "train = read_data(\"../data/train.json\")\n",
    "test  = read_data(\"../data/test.json\")\n",
    "\n",
    "println(@sprintf(\"There are %d rows in the training set\", nrow(train)))\n",
    "println(@sprintf(\"There are %d rows in the test set\", nrow(test)))\n",
    "\n",
    "feature_names = Symbol[\"requester_account_age_in_days_at_request\",\n",
    "                       \"requester_days_since_first_post_on_raop_at_request\",\n",
    "                       \"requester_number_of_comments_at_request\",\n",
    "                       \"requester_number_of_comments_in_raop_at_request\",\n",
    "                       \"requester_number_of_posts_at_request\",\n",
    "                       \"requester_number_of_posts_on_raop_at_request\",\n",
    "                       \"requester_number_of_subreddits_at_request\",\n",
    "                       \"requester_upvotes_minus_downvotes_at_request\",\n",
    "                       \"requester_upvotes_plus_downvotes_at_request\",\n",
    "                       \"unix_timestamp_of_request_utc\"]\n",
    "\n",
    "for feature = feature_names\n",
    "    train[feature] = float64(train[feature])\n",
    "    test[feature]  = float64(test[feature])\n",
    "end\n",
    "\n",
    "columns_to_keep = cat(1, feature_names, [:requester_received_pizza])\n",
    "\n",
    "rf = fit(train[columns_to_keep], :requester_received_pizza, classification_forest_options(num_trees=200, display=true))\n",
    "println(\"\")\n",
    "println(rf)\n",
    "println(\"\")\n",
    "predictions = predict_probs(rf, test)[:,2]\n",
    "submission = DataFrame(request_id=test[:request_id], requester_received_pizza=predictions)\n",
    "writetable(\"simple_julia_benchmark.csv\", submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "['giver_username_if_known', 'number_of_downvotes_of_request_at_retrieval', 'number_of_upvotes_of_request_at_retrieval', 'post_was_edited', 'request_id', 'request_number_of_comments_at_retrieval', 'request_text', 'request_text_edit_aware', 'request_title', 'requester_account_age_in_days_at_request', 'requester_account_age_in_days_at_retrieval', 'requester_days_since_first_post_on_raop_at_request', 'requester_days_since_first_post_on_raop_at_retrieval', 'requester_number_of_comments_at_request', 'requester_number_of_comments_at_retrieval', 'requester_number_of_comments_in_raop_at_request', 'requester_number_of_comments_in_raop_at_retrieval', 'requester_number_of_posts_at_request', 'requester_number_of_posts_at_retrieval', 'requester_number_of_posts_on_raop_at_request', 'requester_number_of_posts_on_raop_at_retrieval', 'requester_number_of_subreddits_at_request', 'requester_received_pizza', 'requester_subreddits_at_request', 'requester_upvotes_minus_downvotes_at_request', 'requester_upvotes_minus_downvotes_at_retrieval', 'requester_upvotes_plus_downvotes_at_request', 'requester_upvotes_plus_downvotes_at_retrieval', 'requester_user_flair', 'requester_username', 'unix_timestamp_of_request', 'unix_timestamp_of_request_utc']\n",
      "(4040, 32)\n",
      "(4040,)\n",
      "(1631, 17)\n",
      "['giver_username_if_known', 'request_id', 'request_text_edit_aware', 'request_title', 'requester_account_age_in_days_at_request', 'requester_days_since_first_post_on_raop_at_request', 'requester_number_of_comments_at_request', 'requester_number_of_comments_in_raop_at_request', 'requester_number_of_posts_at_request', 'requester_number_of_posts_on_raop_at_request', 'requester_number_of_subreddits_at_request', 'requester_subreddits_at_request', 'requester_upvotes_minus_downvotes_at_request', 'requester_upvotes_plus_downvotes_at_request', 'requester_username', 'unix_timestamp_of_request', 'unix_timestamp_of_request_utc']\n"
     ]
    }
   ],
   "source": [
    "# Load raw data and create labels\n",
    "raw_train = pd.read_json('./data/train.json')\n",
    "raw_test = pd.read_json('./data/test.json')\n",
    "\n",
    "# Summarize raw data\n",
    "col_names = list(raw_train.columns.values)\n",
    "print(col_names)\n",
    "print(raw_train.shape)\n",
    "print(train_labels.shape)\n",
    "print(raw_test.shape)\n",
    "#print(dev_data.shape)\n",
    "print(list(raw_test.columns.values))\n",
    "#print(dev_size)\n",
    "#print(train_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3636, 31) (3636,)\n",
      "(404, 31) (404,)\n"
     ]
    }
   ],
   "source": [
    "# split labels and development set\n",
    "train_labels = raw_train[\"requester_received_pizza\"]\n",
    "train_data = raw_train.drop('requester_received_pizza', 1)\n",
    "\n",
    "dev_size = int(round(train_data.shape[0]*.1))\n",
    "\n",
    "mini_train_data, mini_train_labels = train_data[dev_size:], train_labels[dev_size:]\n",
    "print(mini_train_data.shape, mini_train_labels.shape)\n",
    "dev_data, dev_labels = train_data[:dev_size], train_labels[:dev_size]\n",
    "print(dev_data.shape, dev_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature development:**\n",
    "- request_title: parse word vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "#Here we split the number variables from the object or string type variables\n",
    "obj_columns = ['giver_username_if_known','request_id','request_text','request_text_edit_aware','request_title',\n",
    "              'requester_subreddits_at_request','requester_user_flair','requester_username']\n",
    "num_columns = [i for i in col_names if i not in obj_columns]\n",
    "\n",
    "#Here we split the test data columns into\n",
    "test_names = list(test_data.columns.values)\n",
    "test_num_columns = [i for i in test_names if i not in obj_columns]\n",
    "test_obj_columns = [i for i in test_names if i not in test_num_columns]\n",
    "\n",
    "print(len(test_num_columns))\n",
    "print(len(test_obj_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train: (3636, 11) (3636,)\n",
      "shape of dev: (404, 11) (404,)\n",
      "0.752475247525\n"
     ]
    }
   ],
   "source": [
    "#Build an initial model based on number value columns\n",
    "lr = LogisticRegression()\n",
    "print(\"shape of train:\",mini_train_data[test_num_columns].shape, mini_train_labels.shape)\n",
    "print(\"shape of dev:\",dev_data[test_num_columns].shape, dev_labels.shape)\n",
    "lr.fit(mini_train_data[test_num_columns], mini_train_labels)\n",
    "\n",
    "dev_preds = lr.predict(dev_data[test_num_columns])\n",
    "print(metrics.f1_score(dev_labels, dev_preds, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11566\n",
      "0.740099009901\n"
     ]
    }
   ],
   "source": [
    "#Try looking at request only\n",
    "train_text = mini_train_data['request_text_edit_aware']\n",
    "dev_text = dev_data['request_text_edit_aware']\n",
    "vec = CountVectorizer()\n",
    "train_feats = vec.fit_transform(train_text)\n",
    "train_vocab = vec.get_feature_names()\n",
    "print(len(train_vocab))\n",
    "vec2 = CountVectorizer(vocabulary=train_vocab)\n",
    "dev_feats = vec2.transform(dev_text)\n",
    "\n",
    "nb =  MultinomialNB()\n",
    "nb.fit(train_feats, mini_train_labels)\n",
    "nb_preds = nb.predict(dev_feats)\n",
    "print(metrics.f1_score(dev_labels, nb_preds, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107160\n",
      "0.752475247525\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Try looking at request BIGRAMS only\n",
    "train_text = mini_train_data['request_text_edit_aware']\n",
    "dev_text = dev_data['request_text_edit_aware']\n",
    "vec = CountVectorizer(analyzer=\"word\", ngram_range=(1,2))\n",
    "train_feats = vec.fit_transform(train_text)\n",
    "train_vocab = vec.get_feature_names()\n",
    "print(len(train_vocab))\n",
    "vec2 = CountVectorizer(vocabulary=train_vocab)\n",
    "dev_feats = vec2.transform(dev_text)\n",
    "\n",
    "nb =  MultinomialNB()\n",
    "nb.fit(train_feats, mini_train_labels)\n",
    "nb_preds = nb.predict(dev_feats)\n",
    "print(metrics.f1_score(dev_labels, nb_preds, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24109\n",
      "0.752475247525\n"
     ]
    }
   ],
   "source": [
    "#Try looking at request TITLE only\n",
    "train_text = mini_train_data['request_title']\n",
    "dev_text = dev_data['request_title']\n",
    "vec = CountVectorizer(analyzer=\"word\", ngram_range=(1,2))\n",
    "train_feats = vec.fit_transform(train_text)\n",
    "train_vocab = vec.get_feature_names()\n",
    "print(len(train_vocab))\n",
    "vec2 = CountVectorizer(vocabulary=train_vocab)\n",
    "dev_feats = vec2.transform(dev_text)\n",
    "\n",
    "nb =  MultinomialNB()\n",
    "nb.fit(train_feats, mini_train_labels)\n",
    "nb_preds = nb.predict(dev_feats)\n",
    "print(metrics.f1_score(dev_labels, nb_preds, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11566\n",
      "0.752475247525\n"
     ]
    }
   ],
   "source": [
    "#Try looking at request only with TfidfVectorizer\n",
    "train_text = mini_train_data['request_text_edit_aware']\n",
    "dev_text = dev_data['request_text_edit_aware']\n",
    "vec = TfidfVectorizer()\n",
    "train_feats = vec.fit_transform(train_text)\n",
    "train_vocab = vec.get_feature_names()\n",
    "print(len(train_vocab))\n",
    "vec2 = CountVectorizer(vocabulary=train_vocab)\n",
    "dev_feats = vec2.transform(dev_text)\n",
    "\n",
    "nb =  MultinomialNB()\n",
    "nb.fit(train_feats, mini_train_labels)\n",
    "nb_preds = nb.predict(dev_feats)\n",
    "print(metrics.f1_score(dev_labels, nb_preds, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107160\n",
      "0.752475247525\n"
     ]
    }
   ],
   "source": [
    "#Try looking at request only with TfidfVectorizer with BIGRAMS\n",
    "train_text = mini_train_data['request_text_edit_aware']\n",
    "dev_text = dev_data['request_text_edit_aware']\n",
    "vec = TfidfVectorizer(analyzer=\"word\", ngram_range=(1,2))\n",
    "train_feats = vec.fit_transform(train_text)\n",
    "train_vocab = vec.get_feature_names()\n",
    "print(len(train_vocab))\n",
    "vec2 = CountVectorizer(vocabulary=train_vocab)\n",
    "dev_feats = vec2.transform(dev_text)\n",
    "\n",
    "nb =  MultinomialNB()\n",
    "nb.fit(train_feats, mini_train_labels)\n",
    "nb_preds = nb.predict(dev_feats)\n",
    "print(metrics.f1_score(dev_labels, nb_preds, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 11566)\n",
      "[[ 9406  7114  1719 10297 11497  1238  5516  6370 11157 11400  4846  5526\n",
      "   5295  7635  7056  4159  6745 10247   776 10405]\n",
      " [11497  7114  9406 10297  5516  1719  6370  1238 11400 11157  7635  4846\n",
      "   5526  5295  7056  4159  6745 10247   776 10405]]\n",
      "[ 9406  7114  1719 10297 11497  1238  5516  6370 11157 11400  4846  5526\n",
      "  5295  7635  7056  4159  6745 10247   776 10405]\n",
      "so\n",
      "on\n",
      "but\n",
      "this\n",
      "you\n",
      "be\n",
      "is\n",
      "me\n",
      "we\n",
      "would\n",
      "have\n",
      "it\n",
      "in\n",
      "pizza\n",
      "of\n",
      "for\n",
      "my\n",
      "the\n",
      "and\n",
      "to\n",
      "**********\n",
      "you\n",
      "on\n",
      "so\n",
      "this\n",
      "is\n",
      "but\n",
      "me\n",
      "be\n",
      "would\n",
      "we\n",
      "pizza\n",
      "have\n",
      "it\n",
      "in\n",
      "of\n",
      "for\n",
      "my\n",
      "the\n",
      "and\n",
      "to\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# feature_log_prob_\n",
    "print(nb.feature_log_prob_.shape)\n",
    "max_weights = np.argsort(nb.feature_log_prob_, axis=1)[:,-20:]\n",
    "print(max_weights)\n",
    "print(max_weights[0])\n",
    "for i in max_weights[0]:\n",
    "    print(train_vocab[i])\n",
    "print(\"**********\")\n",
    "for i in max_weights[1]:\n",
    "    print(train_vocab[i])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
