{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pizza Project**\n",
    "\n",
    "Andrew Mamroth,\n",
    "Colby Carter,\n",
    "Matt Adereth,\n",
    "Rob Deng\n",
    "\n",
    "\n",
    "Kaggle: https://www.kaggle.com/c/random-acts-of-pizza\n",
    "\n",
    "Team Github: https://github.com/mamrotha/2017_Fall_207_KaggleProj\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data fields:**\n",
    "\n",
    "\"**giver_username_if_known**\": Reddit username of giver if known, i.e. the person satisfying the request (\"N/A\" otherwise).\n",
    "\n",
    "\"number_of_downvotes_of_request_at_retrieval\": Number of downvotes at the time the request was collected.\n",
    "\n",
    "\"number_of_upvotes_of_request_at_retrieval\": Number of upvotes at the time the request was collected.\n",
    "\n",
    "\"post_was_edited\": Boolean indicating whether this post was edited (from Reddit).\n",
    "\n",
    "\"request_id\": Identifier of the post on Reddit, e.g. \"t3_w5491\".\n",
    "\n",
    "\"request_number_of_comments_at_retrieval\": Number of comments for the request at time of retrieval.\n",
    "\n",
    "\"request_text\": Full text of the request.\n",
    "\n",
    "\"request_text_edit_aware\": Edit aware version of \"request_text\". We use a set of rules to strip edited comments indicating the success of the request such as \"EDIT: Thanks /u/foo, the pizza was delicous\".\n",
    "\n",
    "\"request_title\": Title of the request.\n",
    "\n",
    "\"requester_account_age_in_days_at_request\": Account age of requester in days at time of request.\n",
    "\n",
    "\"requester_account_age_in_days_at_retrieval\": Account age of requester in days at time of retrieval.\n",
    "\n",
    "\"requester_days_since_first_post_on_raop_at_request\": Number of days between requesters first post on RAOP and this request (zero if requester has never posted before on RAOP).\n",
    "\n",
    "\"requester_days_since_first_post_on_raop_at_retrieval\": Number of days between requesters first post on RAOP and time of retrieval.\n",
    "\n",
    "\"requester_number_of_comments_at_request\": Total number of comments on Reddit by requester at time of request.\n",
    "\n",
    "\"requester_number_of_comments_at_retrieval\": Total number of comments on Reddit by requester at time of retrieval.\n",
    "\n",
    "\"requester_number_of_comments_in_raop_at_request\": Total number of comments in RAOP by requester at time of request.\n",
    "\n",
    "\"requester_number_of_comments_in_raop_at_retrieval\": Total number of comments in RAOP by requester at time of retrieval.\n",
    "\n",
    "\"requester_number_of_posts_at_request\": Total number of posts on Reddit by requester at time of request.\n",
    "\n",
    "\"requester_number_of_posts_at_retrieval\": Total number of posts on Reddit by requester at time of retrieval.\n",
    "\n",
    "\"requester_number_of_posts_on_raop_at_request\": Total number of posts in RAOP by requester at time of request.\n",
    "\n",
    "\"requester_number_of_posts_on_raop_at_retrieval\": Total number of posts in RAOP by requester at time of retrieval.\n",
    "\n",
    "\"requester_number_of_subreddits_at_request\": The number of subreddits in which the author had already posted in at the time of request.\n",
    "\n",
    "\"requester_received_pizza\": Boolean indicating the success of the request, i.e., whether the requester received pizza.\n",
    "\n",
    "\"requester_subreddits_at_request\": The list of subreddits in which the author had already posted in at the time of request.\n",
    "\n",
    "\"requester_upvotes_minus_downvotes_at_request\": Difference of total upvotes and total downvotes of requester at time of request.\n",
    "\n",
    "\"requester_upvotes_minus_downvotes_at_retrieval\": Difference of total upvotes and total downvotes of requester at time of retrieval.\n",
    "\n",
    "\"requester_upvotes_plus_downvotes_at_request\": Sum of total upvotes and total downvotes of requester at time of request.\n",
    "\n",
    "\"requester_upvotes_plus_downvotes_at_retrieval\": Sum of total upvotes and total downvotes of requester at time of retrieval.\n",
    "\n",
    "\"**requester_user_flair**\": Users on RAOP receive badges (Reddit calls them flairs) which is a small picture next to their username. In our data set the user flair is either None (neither given nor received pizza, N=4282), \"shroom\" (received pizza, but not given, N=1306), or \"PIF\" (pizza given after having received, N=83).\n",
    "\n",
    "\"requester_username\": Reddit username of requester.\n",
    "\n",
    "\"unix_timestamp_of_request\": Unix timestamp of request (supposedly in timezone of user, but in most cases it is equal to the UTC timestamp -- which is incorrect since most RAOP users are from the USA).\n",
    "\n",
    "\"unix_timestamp_of_request_utc\": Unit timestamp of request in UTC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ADD METRICS\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#NLTK - NLP Tokenizing and Cleaning\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4040, 32)\n",
      "['giver_username_if_known', 'number_of_downvotes_of_request_at_retrieval', 'number_of_upvotes_of_request_at_retrieval', 'post_was_edited', 'request_id', 'request_number_of_comments_at_retrieval', 'request_text', 'request_text_edit_aware', 'request_title', 'requester_account_age_in_days_at_request', 'requester_account_age_in_days_at_retrieval', 'requester_days_since_first_post_on_raop_at_request', 'requester_days_since_first_post_on_raop_at_retrieval', 'requester_number_of_comments_at_request', 'requester_number_of_comments_at_retrieval', 'requester_number_of_comments_in_raop_at_request', 'requester_number_of_comments_in_raop_at_retrieval', 'requester_number_of_posts_at_request', 'requester_number_of_posts_at_retrieval', 'requester_number_of_posts_on_raop_at_request', 'requester_number_of_posts_on_raop_at_retrieval', 'requester_number_of_subreddits_at_request', 'requester_received_pizza', 'requester_subreddits_at_request', 'requester_upvotes_minus_downvotes_at_request', 'requester_upvotes_minus_downvotes_at_retrieval', 'requester_upvotes_plus_downvotes_at_request', 'requester_upvotes_plus_downvotes_at_retrieval', 'requester_user_flair', 'requester_username', 'unix_timestamp_of_request', 'unix_timestamp_of_request_utc']\n",
      "(1631, 17)\n",
      "Test columns: ['giver_username_if_known', 'request_id', 'request_text_edit_aware', 'request_title', 'requester_account_age_in_days_at_request', 'requester_days_since_first_post_on_raop_at_request', 'requester_number_of_comments_at_request', 'requester_number_of_comments_in_raop_at_request', 'requester_number_of_posts_at_request', 'requester_number_of_posts_on_raop_at_request', 'requester_number_of_subreddits_at_request', 'requester_subreddits_at_request', 'requester_upvotes_minus_downvotes_at_request', 'requester_upvotes_plus_downvotes_at_request', 'requester_username', 'unix_timestamp_of_request', 'unix_timestamp_of_request_utc']\n"
     ]
    }
   ],
   "source": [
    "# Load raw data and create labels\n",
    "raw_train = pd.read_json('./data/train.json')\n",
    "raw_test = pd.read_json('./data/test.json')\n",
    "\n",
    "# Summarize raw data\n",
    "print(raw_train.shape)\n",
    "print(list(raw_train.columns.values))\n",
    "print(raw_test.shape)\n",
    "print(\"Test columns:\",list(raw_test.columns.values))\n",
    "# no \"retrieval\" variables in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orignial train shape: (4040, 30)\n",
      "test shape: (1631, 17)\n",
      "new shapes: (4040, 28) (1631, 26)\n",
      "(3, 29)\n",
      "Numeric features: ['requester_account_age_in_days_at_request', 'requester_days_since_first_post_on_raop_at_request', 'requester_number_of_comments_at_request', 'requester_number_of_comments_in_raop_at_request', 'requester_number_of_posts_at_request', 'requester_number_of_posts_on_raop_at_request', 'requester_number_of_subreddits_at_request', 'requester_upvotes_minus_downvotes_at_request', 'requester_upvotes_plus_downvotes_at_request', 'unix_timestamp_of_request', 'unix_timestamp_of_request_utc', 'post_length', 'requester_account_age_in_days_at_request_2', 'requester_days_since_first_post_on_raop_at_request_2', 'requester_number_of_comments_at_request_2', 'requester_number_of_comments_in_raop_at_request_2', 'requester_number_of_posts_at_request_2', 'requester_number_of_subreddits_at_request_2', 'requester_upvotes_minus_downvotes_at_request_2', 'post_length_2', 'requester_giver']\n"
     ]
    }
   ],
   "source": [
    "#TRANSFORMATIONS & NEW FEATURES\n",
    "\n",
    "# split labels\n",
    "train_labels = raw_train[\"requester_received_pizza\"]\n",
    "train_data = raw_train.drop(['post_was_edited','requester_received_pizza'], 1) #edits not available in test data\n",
    "test_data = raw_test \n",
    "print(\"orignial train shape:\",train_data.shape)\n",
    "print(\"test shape:\",test_data.shape)\n",
    "\n",
    "\n",
    "#get length of post\n",
    "train_data[\"post_length\"] = train_data[\"request_text_edit_aware\"].apply(lambda x: len(x))\n",
    "test_data[\"post_length\"] = test_data[\"request_text_edit_aware\"].apply(lambda x: len(x))\n",
    "#print(train_data[\"post_length\"].head(6))\n",
    "\n",
    "\n",
    "# Create key quadratic terms for the following numeric variables:\n",
    "# NOTE: variables \"at_retrieval\" are NOT available in the test dataset\n",
    "\n",
    "#remove numeric vars pulled \"at_retreival\"\n",
    "for col in train_data.columns.values:\n",
    "    #print(col)\n",
    "    if \"retrieval\" in col:\n",
    "        train_data = train_data.drop(col, 1)\n",
    "\n",
    "\n",
    "# form quadratic terms to capture curved relationship with log-odds\n",
    "for_quad = [\"requester_account_age_in_days_at_request\", \"requester_days_since_first_post_on_raop_at_request\",\n",
    "            \"requester_number_of_comments_at_request\", \"requester_number_of_comments_in_raop_at_request\",\n",
    "            \"requester_number_of_posts_at_request\", \"requester_number_of_subreddits_at_request\",\n",
    "            \"requester_upvotes_minus_downvotes_at_request\", \"post_length\"]\n",
    "\n",
    "for col in for_quad:\n",
    "    train_data[col + \"_2\"] = train_data[col]**2\n",
    "    test_data[col + \"_2\"] = test_data[col]**2\n",
    "print(\"new shapes:\", train_data.shape, test_data.shape)\n",
    "\n",
    "\n",
    "#transform requester_user_flair into two dummy variables for logistic\n",
    "#TEST DATA DOES NOT CONTAIN THIS VARIABLE\n",
    "#flair = [\"shroom\",\"PIF\"]\n",
    "#for f in flair:\n",
    "    #train_data[f] = train_data[\"requester_user_flair\"].apply(lambda x: 1 if x==f else 0)\n",
    "    #test_data[f] = test_data[\"requester_user_flair\"].apply(lambda x: 1 if x==f else 0)\n",
    "\n",
    "    \n",
    "#flag givers (very few givers who are also requesters)\n",
    "successes = raw_train[raw_train['requester_received_pizza']==True]\n",
    "givers = successes['giver_username_if_known']\n",
    "known_givers = list(givers[givers != 'N/A'])\n",
    "\n",
    "train_data[\"requester_giver\"] = train_data[\"requester_username\"].apply(lambda x: 1 if x in known_givers else 0)\n",
    "test_data[\"requester_giver\"] = test_data[\"requester_username\"].apply(lambda x: 1 if x in known_givers else 0)\n",
    "print(train_data[train_data[\"requester_giver\"] == 1].shape)\n",
    "\n",
    "\n",
    "#Here we split the number variables from the string type variables\n",
    "text_columns = ['giver_username_if_known','request_id','request_text','request_text_edit_aware','request_title',\n",
    "              'requester_subreddits_at_request','requester_user_flair','requester_username']\n",
    "#for logistic regression\n",
    "num_columns = [i for i in train_data.columns.values if i not in text_columns]\n",
    "print(\"Numeric features:\",num_columns)\n",
    "col_names = list(train_data.columns.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini train size: (3434, 29) (3434,)\n",
      "Development set size: (606, 29) (606,)\n"
     ]
    }
   ],
   "source": [
    "# Create mini train and development set\n",
    "dev_size = int(round(train_data.shape[0]*.15))\n",
    "\n",
    "mini_train_data, mini_train_labels = train_data[dev_size:], train_labels[dev_size:]\n",
    "print(\"Mini train size:\",mini_train_data.shape, mini_train_labels.shape)\n",
    "\n",
    "dev_data, dev_labels = train_data[:dev_size], train_labels[:dev_size]\n",
    "print(\"Development set size:\", dev_data.shape, dev_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Colby\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Colby\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Colby\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Colby\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Colby\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Colby\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.naive_bayes.MultinomialNB'> \n",
      "\n",
      " TFIDF =  True \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       1.00      0.74      0.85       606\n",
      "       True       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       1.00      0.74      0.85       606\n",
      "\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'> \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Colby\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1115: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TFIDF =  False \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.96      0.75      0.84       577\n",
      "       True       0.07      0.38      0.12        29\n",
      "\n",
      "avg / total       0.92      0.73      0.81       606\n",
      "\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'> \n",
      "\n",
      " TFIDF =  True \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.99      0.74      0.85       598\n",
      "       True       0.03      0.50      0.05         8\n",
      "\n",
      "avg / total       0.98      0.74      0.84       606\n",
      "\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'> \n",
      "\n",
      " TFIDF =  False \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.86      0.75      0.80       518\n",
      "       True       0.17      0.30      0.21        88\n",
      "\n",
      "avg / total       0.76      0.68      0.72       606\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Rob's Section\n",
    "\n",
    "#Preprocess\n",
    "def nltk_preprocess(data):\n",
    "    stop = stopwords.words('english')\n",
    "    \n",
    "    #Merge title and request text together, lower string\n",
    "    data['title_and_request'] = data[['request_text_edit_aware', 'request_title']].apply(lambda x: ''.join(x), axis=1).str.lower()\n",
    "    #replacing sequences of numbers with a single token, removing various other non-letter characters, removing strings with underscores\n",
    "    data['title_and_request'] = data['title_and_request'].apply(lambda x: re.sub(r'\\d+', r' ', x)).apply(lambda y: re.sub(r'\\W+', r' ', y)).apply(lambda z: re.sub(r\"_+\",r\" \",z))\n",
    "    #NLTK Tokenize\n",
    "    data['tokenized_requests'] = data['title_and_request'].apply(word_tokenize)\n",
    "    #NLTK Remove Stop Words i.e. the, an, etc\n",
    "    data['tokenized_requests'] = data['tokenized_requests'].apply(lambda x: [item for item in x if item not in stop])\n",
    "    #Word count of leftovers; didn't use this yet\n",
    "    data['word_count'] = [len(data['tokenized_requests'][i]) for i in range(data.index[0], data.index[-1]+1)]\n",
    "    #Rejoin after str split\n",
    "    data['tokenized_requests'] = data['tokenized_requests'].apply(lambda x: ' '.join(x))\n",
    "    return data['tokenized_requests']\n",
    "\n",
    "\n",
    "def classify(model, model_parameters = False, use_tfidf=False):\n",
    "    \"\"\"Takes a model and parameters. \n",
    "       Outputs a classification report on the dev data, scored by f1_weighted.\n",
    "       Prints out the best gridsearch parameter of choice.\"\"\"\n",
    "    if(use_tfidf):\n",
    "        pipeliner = Pipeline([('cv', CountVectorizer()),\n",
    "                        ('tfidf', TfidfTransformer()),\n",
    "                        ('model', model())])\n",
    "    else:\n",
    "        pipeliner = Pipeline([(\"cv\", CountVectorizer()), \n",
    "                              (\"model\", model())])\n",
    "\n",
    "        #Make a simple prediction\n",
    "    pipeliner.fit(processed_mini_train_data, mini_train_labels)\n",
    "    pipeliner_pred = pipeliner.predict(processed_dev_data)\n",
    "    print(model, \"\\n\\n\", \"TFIDF = \", use_tfidf, \"\\n\")\n",
    "    print(classification_report(pipeliner_pred, dev_labels))\n",
    "    return None\n",
    "\n",
    "\n",
    "#process text entries\n",
    "processed_mini_train_data = nltk_preprocess(mini_train_data)\n",
    "processed_dev_data = nltk_preprocess(dev_data)\n",
    "processed_test_data = nltk_preprocess(test_data)\n",
    "\n",
    "classify(MultinomialNB, model_parameters = {\"model__analyzer\":\"word\", \"model__ngram_range\":(1,2)}, use_tfidf = True)\n",
    "classify(MultinomialNB, model_parameters = {\"model__analyzer\":\"word\", \"model__ngram_range\":(1,2)}, use_tfidf = False)\n",
    "classify(LogisticRegression, use_tfidf = True)\n",
    "classify(LogisticRegression, use_tfidf = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11509\n",
      "0.681518151815\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.86      0.75      0.80       518\n",
      "       True       0.17      0.30      0.21        88\n",
      "\n",
      "avg / total       0.76      0.68      0.72       606\n",
      "\n",
      "expected\n",
      "mentioned\n",
      "projects\n",
      "redding\n",
      "relatives\n",
      "state\n",
      "denmark\n",
      "christmas\n",
      "gi\n",
      "tucson\n",
      "receive\n",
      "drink\n",
      "including\n",
      "steam\n",
      "lift\n",
      "oatmeal\n",
      "weather\n",
      "ranch\n",
      "loves\n",
      "rather\n",
      "cookout\n",
      "eye\n",
      "except\n",
      "losing\n",
      "topping\n",
      "sunday\n",
      "leg\n",
      "zolo\n",
      "hurting\n",
      "father\n"
     ]
    }
   ],
   "source": [
    "# Explore terms with the largest coefficients\n",
    "\n",
    "def classify2(model, model_parameters = False, use_tfidf=False):\n",
    "    \"\"\"Takes a model and parameters. \n",
    "       Outputs a classification report on the dev data, scored by f1_weighted.\n",
    "       Prints out the best gridsearch parameter of choice.\"\"\"\n",
    "    vec = CountVectorizer()\n",
    "    train_feats = vec.fit_transform(processed_mini_train_data)\n",
    "    train_vocab = vec.get_feature_names()\n",
    "    print(len(train_vocab))\n",
    "    dev_feats = vec.transform(processed_dev_data)\n",
    "    \n",
    "    lr =  LogisticRegression()\n",
    "    #lr =  LogisticRegression(penalty=\"l1\")\n",
    "    lr.fit(train_feats, mini_train_labels)\n",
    "    lr_preds = lr.predict(dev_feats)\n",
    "    print(metrics.f1_score(dev_labels, lr_preds, average='micro'))\n",
    "    print(classification_report(lr_preds, dev_labels))\n",
    "\n",
    "    coefs = lr.coef_\n",
    "    #print(coefs.shape)\n",
    "    max_coefs = np.argsort(coefs, axis=1)[:,-30:]\n",
    "    #print(max_coefs)\n",
    "    top_features = []\n",
    "    for i in range(max_coefs.shape[1]):\n",
    "        #print(max_coefs[0][i])\n",
    "        print(train_vocab[max_coefs[0][i]])\n",
    "\n",
    "\n",
    "classify2(LogisticRegression, use_tfidf = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab train: (3434, 11336)\n",
      "Numeric train: (3434, 21)\n",
      "Combined: (3434, 11357)\n",
      "Test: (1631, 11357)\n"
     ]
    }
   ],
   "source": [
    "#join numeric and vocabulary features\n",
    "\n",
    "#use preprocessed vocab features\n",
    "vec_full = CountVectorizer(stop_words = \"english\")\n",
    "train_feats = vec_full.fit_transform(processed_mini_train_data)\n",
    "train_vocab = vec_full.get_feature_names()\n",
    "dev_feats = vec_full.transform(processed_dev_data)\n",
    "test_feats = vec_full.transform(processed_test_data)\n",
    "\n",
    "#make vocab arrays\n",
    "train_vocab_ar = train_feats.toarray()\n",
    "dev_vocab_ar = dev_feats.toarray()\n",
    "test_vocab_ar = test_feats.toarray()\n",
    "#new_array = np.array(train_feats)\n",
    "print(\"Vocab train:\", train_vocab_ar.shape)\n",
    "\n",
    "#make numeric arrays\n",
    "train_num_ar = mini_train_data[num_columns].as_matrix()\n",
    "dev_num_ar = dev_data[num_columns].as_matrix()\n",
    "test_num_ar = test_data[num_columns].as_matrix()\n",
    "print(\"Numeric train:\",train_num_ar.shape)\n",
    "\n",
    "#join arrays into final feature sets\n",
    "combined_train_feats = np.concatenate((train_vocab_ar,train_num_ar), axis = 1)\n",
    "combined_dev_feats = np.concatenate((dev_vocab_ar,dev_num_ar), axis = 1)\n",
    "combined_test_feats = np.concatenate((test_vocab_ar,test_num_ar), axis = 1)\n",
    "print(\"Combined:\",combined_train_feats.shape)\n",
    "print(\"Test:\",combined_test_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.737623762376\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.94      0.76      0.84       550\n",
      "       True       0.17      0.48      0.25        56\n",
      "\n",
      "avg / total       0.86      0.74      0.79       606\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Build a logistic model based on number value columns + preprocessed text field\n",
    "# test with various values for C\n",
    "full_lr = LogisticRegression(penalty=\"l1\", C = .5)\n",
    "full_lr.fit(combined_train_feats, mini_train_labels)\n",
    "#full_lr.fit(train_feats, mini_train_labels)\n",
    "\n",
    "#print(full_lr.coef_[0:20])\n",
    "\n",
    "dev_preds = full_lr.predict(combined_dev_feats)\n",
    "test_preds = full_lr.predict(combined_test_feats)\n",
    "#dev_preds = full_lr.predict(dev_feats)\n",
    "print(metrics.f1_score(dev_labels, dev_preds, average='micro'))\n",
    "print(classification_report(dev_preds, dev_labels))\n",
    "\n",
    "#test_preds = full_lr.predict(combined_test_feats)\n",
    "\n",
    "test_out = pd.DataFrame()\n",
    "test_out['request_id'] = test_data['request_id']\n",
    "test_out['requester_received_pizza'] = test_preds.astype(int)\n",
    "#num = sum(preds['requester_received_pizza'])\n",
    "\n",
    "#print(num, sum(train_labels))\n",
    "\n",
    "test_out.to_csv('./data/submission2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11244\n",
      "0.729372937294\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.97      0.74      0.84       589\n",
      "       True       0.03      0.29      0.06        17\n",
      "\n",
      "avg / total       0.95      0.73      0.82       606\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Try looking at request only\n",
    "train_text = mini_train_data['request_text_edit_aware']\n",
    "dev_text = dev_data['request_text_edit_aware']\n",
    "vec = CountVectorizer()\n",
    "train_feats = vec.fit_transform(train_text)\n",
    "train_vocab = vec.get_feature_names()\n",
    "print(len(train_vocab))\n",
    "vec2 = CountVectorizer(vocabulary=train_vocab)\n",
    "dev_feats = vec2.transform(dev_text)\n",
    "\n",
    "nb =  MultinomialNB()\n",
    "nb.fit(train_feats, mini_train_labels)\n",
    "nb_preds = nb.predict(dev_feats)\n",
    "print(metrics.f1_score(dev_labels, nb_preds, average='micro'))\n",
    "print(classification_report(nb_preds, dev_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103097\n",
      "0.740924092409\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       1.00      0.74      0.85       606\n",
      "       True       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       1.00      0.74      0.85       606\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Colby\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1115: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#Try looking at request BIGRAMS only\n",
    "train_text = mini_train_data['request_text_edit_aware']\n",
    "dev_text = dev_data['request_text_edit_aware']\n",
    "vec = CountVectorizer(analyzer=\"word\", ngram_range=(1,2))\n",
    "train_feats = vec.fit_transform(train_text)\n",
    "train_vocab = vec.get_feature_names()\n",
    "print(len(train_vocab))\n",
    "vec2 = CountVectorizer(vocabulary=train_vocab)\n",
    "dev_feats = vec2.transform(dev_text)\n",
    "\n",
    "nb =  MultinomialNB()\n",
    "nb.fit(train_feats, mini_train_labels)\n",
    "nb_preds = nb.predict(dev_feats)\n",
    "print(metrics.f1_score(dev_labels, nb_preds, average='micro'))\n",
    "print(classification_report(nb_preds, dev_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23071\n",
      "0.742574257426\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       1.00      0.74      0.85       605\n",
      "       True       0.01      1.00      0.01         1\n",
      "\n",
      "avg / total       1.00      0.74      0.85       606\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Try looking at request TITLE only\n",
    "train_text = mini_train_data['request_title']\n",
    "dev_text = dev_data['request_title']\n",
    "vec = CountVectorizer(analyzer=\"word\", ngram_range=(1,2))\n",
    "train_feats = vec.fit_transform(train_text)\n",
    "train_vocab = vec.get_feature_names()\n",
    "print(len(train_vocab))\n",
    "vec2 = CountVectorizer(vocabulary=train_vocab)\n",
    "dev_feats = vec2.transform(dev_text)\n",
    "\n",
    "nb =  MultinomialNB()\n",
    "nb.fit(train_feats, mini_train_labels)\n",
    "nb_preds = nb.predict(dev_feats)\n",
    "print(metrics.f1_score(dev_labels, nb_preds, average='micro'))\n",
    "print(classification_report(nb_preds, dev_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11244\n",
      "0.740924092409\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       1.00      0.74      0.85       606\n",
      "       True       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       1.00      0.74      0.85       606\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Colby\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1115: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#Try looking at request only with TfidfVectorizer\n",
    "train_text = mini_train_data['request_text_edit_aware']\n",
    "dev_text = dev_data['request_text_edit_aware']\n",
    "vec = TfidfVectorizer()\n",
    "train_feats = vec.fit_transform(train_text)\n",
    "train_vocab = vec.get_feature_names()\n",
    "print(len(train_vocab))\n",
    "vec2 = CountVectorizer(vocabulary=train_vocab)\n",
    "dev_feats = vec2.transform(dev_text)\n",
    "\n",
    "nb =  MultinomialNB()\n",
    "nb.fit(train_feats, mini_train_labels)\n",
    "nb_preds = nb.predict(dev_feats)\n",
    "print(metrics.f1_score(dev_labels, nb_preds, average='micro'))\n",
    "print(classification_report(nb_preds, dev_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 11566)\n",
      "[[ 9406  7114  1719 10297 11497  1238  5516  6370 11157 11400  4846  5526\n",
      "   5295  7635  7056  4159  6745 10247   776 10405]\n",
      " [11497  7114  9406 10297  5516  1719  6370  1238 11400 11157  7635  4846\n",
      "   5526  5295  7056  4159  6745 10247   776 10405]]\n",
      "[ 9406  7114  1719 10297 11497  1238  5516  6370 11157 11400  4846  5526\n",
      "  5295  7635  7056  4159  6745 10247   776 10405]\n",
      "so\n",
      "on\n",
      "but\n",
      "this\n",
      "you\n",
      "be\n",
      "is\n",
      "me\n",
      "we\n",
      "would\n",
      "have\n",
      "it\n",
      "in\n",
      "pizza\n",
      "of\n",
      "for\n",
      "my\n",
      "the\n",
      "and\n",
      "to\n",
      "**********\n",
      "you\n",
      "on\n",
      "so\n",
      "this\n",
      "is\n",
      "but\n",
      "me\n",
      "be\n",
      "would\n",
      "we\n",
      "pizza\n",
      "have\n",
      "it\n",
      "in\n",
      "of\n",
      "for\n",
      "my\n",
      "the\n",
      "and\n",
      "to\n"
     ]
    }
   ],
   "source": [
    "#NB weights of vocab without preprocessing step\n",
    "# feature_log_prob_\n",
    "print(nb.feature_log_prob_.shape)\n",
    "max_weights = np.argsort(nb.feature_log_prob_, axis=1)[:,-20:]\n",
    "print(max_weights)\n",
    "print(max_weights[0])\n",
    "for i in max_weights[0]:\n",
    "    print(train_vocab[i])\n",
    "print(\"**********\")\n",
    "for i in max_weights[1]:\n",
    "    print(train_vocab[i])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
